{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning our BERT-based model for NAT Prediction\n",
    "\n",
    "We identified optimal hyperparameters using a Bayesian search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Tuple, Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from smart_open import open\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from transformers import BertConfig, get_linear_schedule_with_warmup\n",
    "import wandb\n",
    "\n",
    "from common.constants import (\n",
    "    S3_FINETUNE_UNRESTRICTED_PREPROCESSED_PATH,\n",
    "    S3_MODEL_OUTPUT_PATH,\n",
    ")\n",
    "from common.models import BertFinetune\n",
    "from common.datasets import FinetuneDataset\n",
    "from common.utilities import load_vocab, create_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create training and evaluation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_finetune(\n",
    "    model: BertFinetune, dataloader: DataLoader, device: torch.device\n",
    ") -> Tuple[float]:\n",
    "    \"\"\"Evaluate model on validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Keep running values for loss, logits, and labels\n",
    "    eval_loss = 0\n",
    "    eval_logits, eval_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {key: seq.to(device) for key, seq in batch.items()}\n",
    "\n",
    "            outputs = model(\n",
    "                feature_ids=batch[\"feature_tokens\"],\n",
    "                time_ids=batch[\"time_from_prediction_tokens\"],\n",
    "                code_type_ids=batch[\"code_type_tokens\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"label\"],\n",
    "            )\n",
    "\n",
    "            eval_loss += outputs[0].item()\n",
    "            eval_logits += torch.squeeze(outputs[1]).tolist()\n",
    "            eval_labels += torch.squeeze(batch[\"label\"]).tolist()\n",
    "\n",
    "    # Calculate metrics\n",
    "    eval_loss /= len(dataloader)\n",
    "    eval_auroc = roc_auc_score(y_true=eval_labels, y_score=eval_logits)\n",
    "    eval_auprc = average_precision_score(\n",
    "        y_true=eval_labels, y_score=eval_logits\n",
    "    )\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return eval_loss, eval_auroc, eval_auprc\n",
    "\n",
    "\n",
    "def training(\n",
    "    model: BertFinetune,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler,\n",
    "    device: torch.device,\n",
    "    config: wandb.Config,\n",
    "):\n",
    "    \"\"\"Model training loop.\"\"\"\n",
    "    best_val_auroc = 0\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        # Track performance on segments of the traning dataset\n",
    "        running_loss = 0\n",
    "        running_logits, running_labels = [], []\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {key: seq.to(device) for key, seq in batch.items()}\n",
    "\n",
    "            outputs = model(\n",
    "                feature_ids=batch[\"feature_tokens\"],\n",
    "                time_ids=batch[\"time_from_prediction_tokens\"],\n",
    "                code_type_ids=batch[\"code_type_tokens\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                labels=batch[\"label\"],\n",
    "            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Track training statistics\n",
    "            running_loss += loss.item()\n",
    "            running_logits += torch.squeeze(outputs[1]).tolist()\n",
    "            running_labels += torch.squeeze(batch[\"label\"]).tolist()\n",
    "\n",
    "            # Normalize for gradient accumulation\n",
    "            loss = loss / config.num_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize with accumulated gradients\n",
    "            if (i + 1) % config.num_accumulation_steps == 0:\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Evaluate on validation dataset\n",
    "            if (i + 1) % config.log_steps == 0:\n",
    "                val_loss, val_auroc, val_auprc = evaluate_finetune(\n",
    "                    model=model,\n",
    "                    dataloader=val_dataloader,\n",
    "                    device=device,\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"steps\": i,\n",
    "                        \"train_loss\": running_loss / config.log_steps,\n",
    "                        \"train_auroc\": roc_auc_score(\n",
    "                            y_true=running_labels, y_score=running_logits\n",
    "                        ),\n",
    "                        \"train_auprc\": average_precision_score(\n",
    "                            y_true=running_labels, y_score=running_logits\n",
    "                        ),\n",
    "                        \"val_loss\": val_loss,\n",
    "                        \"val_auroc\": val_auroc,\n",
    "                        \"val_auprc\": val_auprc,\n",
    "                    }\n",
    "                )\n",
    "                running_loss = 0\n",
    "                running_logits, running_labels = [], []\n",
    "\n",
    "                # Save model if performance improved\n",
    "                if val_auroc > best_val_auroc:\n",
    "                    wandb_save_path = os.path.join(\n",
    "                        S3_MODEL_OUTPUT_PATH,\n",
    "                        \"finetuned\",\n",
    "                        wandb.run.name + \".pt\",\n",
    "                    )\n",
    "                    with open(wandb_save_path, \"wb\") as f:\n",
    "                        torch.save(model.state_dict(), f)\n",
    "                    print(\n",
    "                        f\"Saved model weights (with val auroc of {val_auroc}) to:\",\n",
    "                        wandb_save_path,\n",
    "                    )\n",
    "                    best_val_auroc = val_auroc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create finetuning pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def finetune_pipeline(config: Dict[str, Union[int, float]]):\n",
    "    \"\"\"Run finetuning pipeline.\"\"\"\n",
    "    with wandb.init(project=\"transformer-nat-finetune\", config=config):\n",
    "        try:\n",
    "            # Configuration is filled out by WandB according to the sweep configuration\n",
    "            config = wandb.config\n",
    "            device = torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "\n",
    "            # Load vocabulary encoders\n",
    "            feature_vocab = load_vocab(\n",
    "                \"feature_vocab_with_cls_pad_mask.pickle\"\n",
    "            )\n",
    "            time_vocab = load_vocab(\"time_vocab_with_pad.pickle\")\n",
    "            code_type_vocab = load_vocab(\"type_vocab_with_pad.pickle\")\n",
    "\n",
    "            # Load and create training dataloader\n",
    "            train_dataloader = create_dataloader(\n",
    "                dataset_path=os.path.join(\n",
    "                    S3_FINETUNE_UNRESTRICTED_PREPROCESSED_PATH, \"train.parquet\"\n",
    "                ),\n",
    "                dataset_constructor=FinetuneDataset,\n",
    "                feature_vocab=feature_vocab,\n",
    "                time_vocab=time_vocab,\n",
    "                code_type_vocab=code_type_vocab,\n",
    "                config=config,\n",
    "                truncate_to=-1,\n",
    "                weighted_sampling=True,\n",
    "            )\n",
    "\n",
    "            # Load and create validation dataloader\n",
    "            val_dataloader = create_dataloader(\n",
    "                dataset_path=os.path.join(\n",
    "                    S3_FINETUNE_UNRESTRICTED_PREPROCESSED_PATH, \"val.parquet\"\n",
    "                ),\n",
    "                dataset_constructor=FinetuneDataset,\n",
    "                feature_vocab=feature_vocab,\n",
    "                time_vocab=time_vocab,\n",
    "                code_type_vocab=code_type_vocab,\n",
    "                config=config,\n",
    "                truncate_to=-1,\n",
    "                weighted_sampling=False,\n",
    "            )\n",
    "\n",
    "            # Create BERT configuration\n",
    "            bert_config = BertConfig(\n",
    "                # Native configurations\n",
    "                pad_token_id=None,\n",
    "                position_embedding_type=None,\n",
    "                type_vocab_size=None,\n",
    "                vocab_size=None,\n",
    "                max_position_embeddings=None,\n",
    "                # Values tuned by hyperparameter sweep\n",
    "                classifier_dropout=config.classifier_dropout,\n",
    "                hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "                attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "                hidden_size=config.hidden_size,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                num_attention_heads=config.num_attention_heads,\n",
    "                num_hidden_layers=config.num_hidden_layers,\n",
    "                pos_weight=config.pos_weight,\n",
    "                # Custom configurations\n",
    "                feature_vocab_size=len(feature_vocab),\n",
    "                time_vocab_size=len(time_vocab),\n",
    "                code_type_vocab_size=len(code_type_vocab),\n",
    "                feature_pad_id=feature_vocab[\"PAD\"],\n",
    "                time_pad_id=time_vocab[\"PAD\"],\n",
    "                code_type_pad_id=code_type_vocab[\"PAD\"],\n",
    "            )\n",
    "\n",
    "            # Create BERT model\n",
    "            model = BertFinetune(config=bert_config).to(device)\n",
    "\n",
    "            # Load pretrained model weights\n",
    "            with open(config.pretrained_path, \"rb\") as f:\n",
    "                model.load_state_dict(torch.load(f), strict=False)\n",
    "            print(\"Loaded pretrained weights from:\", config.pretrained_path)\n",
    "\n",
    "            # Create Adam optimizer\n",
    "            optimizer = AdamW(\n",
    "                params=model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.adam_weight_decay,\n",
    "            )\n",
    "\n",
    "            # Create learning rate scheduler\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer=optimizer,\n",
    "                num_warmup_steps=config.scheduler_warmup_steps,\n",
    "                num_training_steps=(\n",
    "                    len(train_dataloader) // config.num_accumulation_steps\n",
    "                )\n",
    "                * config.epochs,\n",
    "            )\n",
    "\n",
    "            # Run training loop\n",
    "            training(\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "        # Handle errors without quitting wandb sweep\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc(), file=sys.stderr)\n",
    "            exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian hyperparameter search\n",
    "\n",
    "Parameters tuned were: \n",
    "- `learning_rate`\n",
    "- `adam_weight_decay`\n",
    "- `scheduler_warmup_steps`\n",
    "- `pos_weight`\n",
    "- `classifier_dropout`\n",
    "- `hidden_dropout_prob`\n",
    "- `attention_probs_dropout_prob`\n",
    "- `num_accumulation_steps`\n",
    "- `sample_weight`\n",
    "\n",
    "We used hyperband early termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    'name': 'finetune_sweep',\n",
    "    'method': 'bayes',\n",
    "    'metric': {\n",
    "        'name': 'val_auroc',\n",
    "        'goal': 'maximize'\n",
    "    },\n",
    "    'parameters': {\n",
    "        'epochs': {\n",
    "            'value': 1\n",
    "        },\n",
    "        'max_seq_length': {\n",
    "            'value': 512\n",
    "        },\n",
    "        'log_steps': {\n",
    "            'value': 1000\n",
    "        },\n",
    "        'hidden_size': {\n",
    "            'value': 780\n",
    "        },\n",
    "        'intermediate_size': {\n",
    "            'value': 800\n",
    "        },\n",
    "        'num_attention_heads': {\n",
    "            'value': 3\n",
    "        },\n",
    "        'num_hidden_layers': {\n",
    "            'value': 6\n",
    "        },\n",
    "        'batch_size': {\n",
    "            'value': 32\n",
    "        },\n",
    "        'pretrained_path': {\n",
    "            'value': 's3://transformer-v5/saved_models/pretrained/deep-cosmos-5.pt'\n",
    "        },\n",
    "        'learning_rate': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 5e-6,\n",
    "            'max': 5e-5\n",
    "        },\n",
    "        'adam_weight_decay': {\n",
    "            'distribution': 'log_uniform_values',\n",
    "            'min': 1e-4,\n",
    "            'max': 1e-2\n",
    "        },\n",
    "        'scheduler_warmup_steps': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 10,\n",
    "            'max': 100\n",
    "        },\n",
    "        'pos_weight': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 20,\n",
    "            'max': 80,\n",
    "        },\n",
    "        'classifier_dropout': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.1,\n",
    "            'max': 0.3\n",
    "        },\n",
    "        'hidden_dropout_prob': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.2,\n",
    "            'max': 0.4\n",
    "        },\n",
    "        'attention_probs_dropout_prob': {\n",
    "            'distribution': 'uniform',\n",
    "            'min': 0.2,\n",
    "            'max': 0.4\n",
    "        },\n",
    "        'num_accumulation_steps': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 1,\n",
    "            'max': 10\n",
    "        },\n",
    "        'sample_weight': {\n",
    "            'distribution': 'int_uniform',\n",
    "            'min': 20,\n",
    "            'max': 80\n",
    "        }\n",
    "    },\n",
    "    'early_terminate': {\n",
    "        'type': 'hyperband',\n",
    "        'min_iter': 1,\n",
    "        'eta': 2\n",
    "    }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameter sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project='finetune-sweep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start sweep on this machine\n",
    "wandb.agent(sweep_id, finetune_pipeline, count=50, project='finetune-sweep')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run using optimal hyperparameters\n",
    "\n",
    "After identifying optimal hyperparameters, we ran finetuning again for a longer duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_config = {\n",
    "    \"epochs\": 1,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"log_steps\": 5000,\n",
    "    \"learning_rate\": 0.00002,\n",
    "    \"adam_weight_decay\": 0.001,\n",
    "    \"batch_size\": 32,\n",
    "    \"scheduler_warmup_steps\": 40,\n",
    "    \"hidden_size\": 780,\n",
    "    \"intermediate_size\": 800,\n",
    "    \"num_attention_heads\": 3,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"classifier_dropout\": 0.2,\n",
    "    \"hidden_dropout_prob\": 0.3,\n",
    "    \"attention_probs_dropout_prob\": 0.3,\n",
    "    \"num_accumulation_steps\": 1,\n",
    "    \"pretrained_path\": \"s3://transformer-v5/saved_models/pretrained/deep-cosmos-5.pt\",\n",
    "    \"pos_weight\": 80,\n",
    "    \"sample_weight\": 2,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "finetune_pipeline(config=final_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('aws_neuron_pytorch_p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d7de49559c0dd37a4dec6240f0e3a5162c2d1e7f1c7d5bec34ee2493a8090e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

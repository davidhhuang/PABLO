{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pretraining our BERT-based model\n",
    "We used masked LM and next visit diagnosis prediction objectives. We identified optimal hyperparameters using a Bayesian search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import sys\n",
    "import traceback\n",
    "from typing import Dict, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from smart_open import open\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from tqdm import tqdm\n",
    "from transformers import AdamW, BertConfig, get_linear_schedule_with_warmup\n",
    "\n",
    "import wandb\n",
    "from common.constants import (\n",
    "    S3_MODEL_OUTPUT_PATH,\n",
    "    S3_PRETRAIN_PREPROCESSED_PATH,\n",
    ")\n",
    "from common.datasets import PretrainDataset\n",
    "from common.models import BertPretrain\n",
    "from common.utilities import create_dataloader, load_vocab\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create taining and evaluation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pretrain(\n",
    "    model: BertPretrain, dataloader: DataLoader, device: torch.device\n",
    ") -> Tuple[float]:\n",
    "    \"\"\"Evaluate model on validation dataset.\"\"\"\n",
    "    model.eval()\n",
    "\n",
    "    # Keep running values for loss, logits, and labels\n",
    "    eval_total_loss, eval_mlm_loss, eval_next_visit_loss = 0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader):\n",
    "            batch = {key: seq.to(device) for key, seq in batch.items()}\n",
    "            outputs = model(\n",
    "                feature_ids=batch[\"feature_tokens\"],\n",
    "                time_ids=batch[\"time_from_prediction_tokens\"],\n",
    "                code_type_ids=batch[\"code_type_tokens\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                mask_labels=batch[\"mask_labels\"],\n",
    "                labels=batch[\"label\"],\n",
    "            )\n",
    "            eval_total_loss += outputs[0].item()\n",
    "            eval_mlm_loss += outputs[1].item()\n",
    "            eval_next_visit_loss += outputs[2].item()\n",
    "\n",
    "    # Calculate average loss\n",
    "    eval_total_loss /= len(dataloader)\n",
    "    eval_mlm_loss /= len(dataloader)\n",
    "    eval_next_visit_loss /= len(dataloader)\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    return eval_total_loss, eval_mlm_loss, eval_next_visit_loss\n",
    "\n",
    "\n",
    "def training(\n",
    "    model: BertPretrain,\n",
    "    train_dataloader: DataLoader,\n",
    "    val_dataloader: DataLoader,\n",
    "    optimizer: torch.optim.Optimizer,\n",
    "    scheduler: torch.optim.lr_scheduler,\n",
    "    device: torch.device,\n",
    "    config: wandb.Config,\n",
    "):\n",
    "    \"\"\"Model training loop.\"\"\"\n",
    "    best_val_total_loss = float(\"inf\")\n",
    "\n",
    "    for epoch in range(config.epochs):\n",
    "        # Track performance on segments of the traning dataset\n",
    "        running_total_loss, running_mlm_loss, running_next_visit_loss = 0, 0, 0\n",
    "\n",
    "        for i, batch in enumerate(tqdm(train_dataloader)):\n",
    "            batch = {key: seq.to(device) for key, seq in batch.items()}\n",
    "\n",
    "            outputs = model(\n",
    "                feature_ids=batch[\"feature_tokens\"],\n",
    "                time_ids=batch[\"time_from_prediction_tokens\"],\n",
    "                code_type_ids=batch[\"code_type_tokens\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                mask_labels=batch[\"mask_labels\"],\n",
    "                labels=batch[\"label\"],\n",
    "            )\n",
    "\n",
    "            loss = outputs[0]\n",
    "\n",
    "            # Track training statistics\n",
    "            running_total_loss += loss.item()\n",
    "            running_mlm_loss += outputs[1].item()\n",
    "            running_next_visit_loss += outputs[2].item()\n",
    "\n",
    "            # Normalize for gradient accumulation\n",
    "            loss = loss / config.num_accumulation_steps\n",
    "            loss.backward()\n",
    "\n",
    "            # Optimize with accumulated gradients\n",
    "            if (i + 1) % config.num_accumulation_steps == 0:\n",
    "                # Update Optimizer\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Evaluate on validation dataset\n",
    "            if (i + 1) % config.log_steps == 0:\n",
    "                (\n",
    "                    val_total_loss,\n",
    "                    val_mlm_loss,\n",
    "                    val_next_visit_loss,\n",
    "                ) = evaluate_pretrain(\n",
    "                    model=model,\n",
    "                    dataloader=val_dataloader,\n",
    "                    device=device,\n",
    "                )\n",
    "                wandb.log(\n",
    "                    {\n",
    "                        \"epoch\": epoch,\n",
    "                        \"steps\": i,\n",
    "                        \"train_total_loss\": running_total_loss\n",
    "                        / config.log_steps,\n",
    "                        \"train_mlm_loss\": running_mlm_loss / config.log_steps,\n",
    "                        \"train_next_visit_loss\": running_next_visit_loss\n",
    "                        / config.log_steps,\n",
    "                        \"val_total_loss\": val_total_loss,\n",
    "                        \"val_mlm_loss\": val_mlm_loss,\n",
    "                        \"val_next_visit_loss\": val_next_visit_loss,\n",
    "                    }\n",
    "                )\n",
    "                running_total_loss = 0\n",
    "                running_mlm_loss = 0\n",
    "                running_next_visit_loss = 0\n",
    "\n",
    "                # Save model\n",
    "                if val_total_loss < best_val_total_loss:\n",
    "                    wandb_save_path = os.path.join(\n",
    "                        S3_MODEL_OUTPUT_PATH,\n",
    "                        \"pretrained\",\n",
    "                        wandb.run.name + \".pt\",\n",
    "                    )\n",
    "                    with open(wandb_save_path, \"wb\") as f:\n",
    "                        torch.save(model.state_dict(), f)\n",
    "                    print(\n",
    "                        f\"Saved model weights (with total val loss of {val_total_loss}) to:\",\n",
    "                        wandb_save_path,\n",
    "                    )\n",
    "                    best_val_total_loss = val_total_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create pretraining pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_pipeline(config: Dict[str, Union[int, float]]):\n",
    "    \"\"\"Run pretraining pipeline.\"\"\"\n",
    "    with wandb.init(project=\"ehr-transformer-v5-pretrain\", config=config):\n",
    "        try:\n",
    "            # Configuration is filled out by WandB according to the sweep configuration\n",
    "            config = wandb.config\n",
    "            device = torch.device(\n",
    "                \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "            )\n",
    "\n",
    "            # Load vocabulary encoders\n",
    "            feature_vocab = load_vocab(\n",
    "                \"feature_vocab_with_cls_pad_mask.pickle\"\n",
    "            )\n",
    "            time_vocab = load_vocab(\"time_vocab_with_pad.pickle\")\n",
    "            code_type_vocab = load_vocab(\"type_vocab_with_pad.pickle\")\n",
    "            next_visit_diagnosis_vocab = load_vocab(\n",
    "                \"last_visit_dx_category_vocab.pickle\"\n",
    "            )\n",
    "\n",
    "            # Load and create training dataloader\n",
    "            train_dataloader = create_dataloader(\n",
    "                dataset_path=os.path.join(\n",
    "                    S3_PRETRAIN_PREPROCESSED_PATH, \"train.parquet\"\n",
    "                ),\n",
    "                dataset_constructor=PretrainDataset,\n",
    "                feature_vocab=feature_vocab,\n",
    "                time_vocab=time_vocab,\n",
    "                code_type_vocab=code_type_vocab,\n",
    "                config=config,\n",
    "                truncate_to=-1,\n",
    "            )\n",
    "\n",
    "            # Load and create validation dataloader\n",
    "            val_dataloader = create_dataloader(\n",
    "                dataset_path=os.path.join(\n",
    "                    S3_PRETRAIN_PREPROCESSED_PATH, \"test.parquet\"\n",
    "                ),\n",
    "                dataset_constructor=PretrainDataset,\n",
    "                feature_vocab=feature_vocab,\n",
    "                time_vocab=time_vocab,\n",
    "                code_type_vocab=code_type_vocab,\n",
    "                config=config,\n",
    "                truncate_to=-1,\n",
    "            )\n",
    "\n",
    "            # Create BERT configuration\n",
    "            bert_config = BertConfig(\n",
    "                # Native configurations\n",
    "                pad_token_id=None,\n",
    "                position_embedding_type=None,\n",
    "                type_vocab_size=None,\n",
    "                vocab_size=len(feature_vocab),\n",
    "                max_position_embeddings=None,\n",
    "                # Values tuned by hyperparameter sweep\n",
    "                classifier_dropout=config.classifier_dropout,\n",
    "                hidden_dropout_prob=config.hidden_dropout_prob,\n",
    "                attention_probs_dropout_prob=config.attention_probs_dropout_prob,\n",
    "                hidden_size=config.hidden_size,\n",
    "                intermediate_size=config.intermediate_size,\n",
    "                num_attention_heads=config.num_attention_heads,\n",
    "                num_hidden_layers=config.num_hidden_layers,\n",
    "                # Custom configurations\n",
    "                feature_vocab_size=len(feature_vocab),\n",
    "                time_vocab_size=len(time_vocab),\n",
    "                code_type_vocab_size=len(code_type_vocab),\n",
    "                feature_pad_id=feature_vocab[\"PAD\"],\n",
    "                time_pad_id=time_vocab[\"PAD\"],\n",
    "                code_type_pad_id=code_type_vocab[\"PAD\"],\n",
    "                next_visit_diagnosis_labels_size=len(\n",
    "                    next_visit_diagnosis_vocab\n",
    "                ),\n",
    "            )\n",
    "\n",
    "            # Create BERT model\n",
    "            model = BertPretrain(config=bert_config).to(device)\n",
    "\n",
    "            # Create Adam optimizer\n",
    "            optimizer = AdamW(\n",
    "                params=model.parameters(),\n",
    "                lr=config.learning_rate,\n",
    "                weight_decay=config.adam_weight_decay,\n",
    "            )\n",
    "\n",
    "            # Create learning rate scheduler\n",
    "            scheduler = get_linear_schedule_with_warmup(\n",
    "                optimizer=optimizer,\n",
    "                num_warmup_steps=config.scheduler_warmup_steps,\n",
    "                num_training_steps=len(train_dataloader)\n",
    "                // config.num_accumulation_steps\n",
    "                * config.epochs,\n",
    "            )\n",
    "\n",
    "            # Run training loop\n",
    "            training(\n",
    "                model=model,\n",
    "                train_dataloader=train_dataloader,\n",
    "                val_dataloader=val_dataloader,\n",
    "                optimizer=optimizer,\n",
    "                scheduler=scheduler,\n",
    "                device=device,\n",
    "                config=config,\n",
    "            )\n",
    "\n",
    "        # Handle errors without quitting wandb sweep\n",
    "        except Exception as e:\n",
    "            print(traceback.print_exc(), file=sys.stderr)\n",
    "            exit(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bayesian hyperparameter search\n",
    "\n",
    "Parameters tuned were: \n",
    "- `learning_rate`\n",
    "- `adam_weight_decay`\n",
    "- `hidden_size`\n",
    "- `intermediate_size`\n",
    "- `num_attention_heads`\n",
    "- `num_hidden_layers`\n",
    "- `classifier_dropout`\n",
    "- `hidden_dropout_prob`\n",
    "- `attention_probs_dropout_prob`\n",
    "- `num_accumulation_steps`\n",
    "\n",
    "We used hyperband early termination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sweep_config = {\n",
    "    \"name\": \"pretrain_sweep\",\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\": {\"name\": \"val_total_loss\", \"goal\": \"minimize\"},\n",
    "    \"parameters\": {\n",
    "        \"epochs\": {\"value\": 1},\n",
    "        \"max_seq_length\": {\"value\": 512},\n",
    "        \"log_steps\": {\"value\": 600},\n",
    "        \"learning_rate\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-4,\n",
    "            \"max\": 1e-3,\n",
    "        },\n",
    "        \"adam_weight_decay\": {\n",
    "            \"distribution\": \"log_uniform_values\",\n",
    "            \"min\": 1e-3,\n",
    "            \"max\": 1,\n",
    "        },\n",
    "        \"batch_size\": {\n",
    "            \"value\": 16,\n",
    "        },\n",
    "        \"scheduler_warmup_steps\": {\n",
    "            \"value\": 0,\n",
    "        },\n",
    "        \"hidden_size\": {\n",
    "            # hidden (embedding) size needs to be a multiple of num_attention_heads\n",
    "            \"values\": [420, 540, 600, 660, 720, 780, 840, 900, 960]\n",
    "        },\n",
    "        \"intermediate_size\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 700,\n",
    "            \"max\": 3072,\n",
    "        },\n",
    "        \"num_attention_heads\": {\"values\": [2, 3, 4, 5, 6, 10, 12]},\n",
    "        \"num_hidden_layers\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 3,\n",
    "            \"max\": 8,\n",
    "        },\n",
    "        \"classifier_dropout\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0,\n",
    "            \"max\": 0.3,\n",
    "        },\n",
    "        \"hidden_dropout_prob\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0,\n",
    "            \"max\": 0.3,\n",
    "        },\n",
    "        \"attention_probs_dropout_prob\": {\n",
    "            \"distribution\": \"uniform\",\n",
    "            \"min\": 0,\n",
    "            \"max\": 0.5,\n",
    "        },\n",
    "        \"num_accumulation_steps\": {\n",
    "            \"distribution\": \"int_uniform\",\n",
    "            \"min\": 12,\n",
    "            \"max\": 24,\n",
    "        },\n",
    "    },\n",
    "    \"early_terminate\": {\"type\": \"hyperband\", \"min_iter\": 1, \"eta\": 2},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create hyperparameter sweep\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"pretrain-sweeps\")\n",
    "\n",
    "# Start sweep on this machine\n",
    "wandb.agent(sweep_id, pretrain_pipeline, count=50, project=\"pretrain-sweeps\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run using optimal hyperparameters\n",
    "\n",
    "After identifying optimal hyperparameters, we ran pretraining again for a longer duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_config = {\n",
    "    \"epochs\": 5,\n",
    "    \"max_seq_length\": 512,\n",
    "    \"log_steps\": 20000,\n",
    "    \"learning_rate\": 4e-4,\n",
    "    \"adam_weight_decay\": 0.5,\n",
    "    \"batch_size\": 32,\n",
    "    \"scheduler_warmup_steps\": 0,\n",
    "    \"hidden_size\": 780,\n",
    "    \"intermediate_size\": 800,\n",
    "    \"num_attention_heads\": 3,\n",
    "    \"num_hidden_layers\": 6,\n",
    "    \"classifier_dropout\": 0.1,\n",
    "    \"hidden_dropout_prob\": 0.2,\n",
    "    \"attention_probs_dropout_prob\": 0.1,\n",
    "    \"num_accumulation_steps\": 7,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrain_pipeline(config=final_config)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('aws_neuron_pytorch_p37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "1d7de49559c0dd37a4dec6240f0e3a5162c2d1e7f1c7d5bec34ee2493a8090e4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
